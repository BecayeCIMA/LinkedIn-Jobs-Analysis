{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Job info from LinkedIn\n",
    "\n",
    "<a href=\"https://medium.com/nerd-for-tech/linked-in-web-scraper-using-selenium-15189959b3ba\">Tutorial here.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from bot.Scraper import Scraper\n",
    "from src.database.csv import save_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FOLDER = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scraper object\n",
    "scraper = Scraper(delay=1)\n",
    "\n",
    "# Login to LinkedIn\n",
    "scraper.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search jobs by title and location\n",
    "job_title = 'data scientist'\n",
    "job_location = 'canada'\n",
    "scraper.search_jobs(job_title, job_location)\n",
    "\n",
    "# Get the  pagination buttons\n",
    "time.sleep(2)\n",
    "pagination_buttons = scraper.get_pagination_buttons()\n",
    "\n",
    "jobs = scraper.get_current_page_jobs()\n",
    "pagination = scraper.driver.find_element(By.CLASS_NAME, \"jobs-search-pagination__pages\")\n",
    "pagination_buttons = pagination.find_elements(By.XPATH, './/button')\n",
    "pagination_buttons[0].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Job Data\n",
    "\n",
    "Search for jobs and loop through all pages to get the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_job_data(job_title, job_location, delay=1):\n",
    "    \n",
    "    # Search jobs by title and location\n",
    "    scraper.search_jobs(job_title, job_location)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Get the  pagination buttons\n",
    "    pagination_buttons = scraper.get_pagination_buttons()\n",
    "    print(f\"{len(pagination_buttons)} pages found\")\n",
    "\n",
    "    # Loop through each page\n",
    "    # [:-1] is to ignore the last page as we click on the i+1 button\n",
    "    all_jobs = []\n",
    "    for i, button in enumerate(pagination_buttons[:-1]):\n",
    "\n",
    "        try:\n",
    "            print(\"button\", button)\n",
    "            # get the jobs of the current page\n",
    "            current_page_jobs = scraper.get_current_page_jobs()\n",
    "            all_jobs.append(current_page_jobs)\n",
    "\n",
    "            # navigate to the next page\n",
    "            # button.click()\n",
    "            # Get the buttons again to avoid a StaleElementReferenceException\n",
    "            pagination_buttons = scraper.get_pagination_buttons()\n",
    "            pagination_buttons[i+1].click()\n",
    "\n",
    "        except StaleElementReferenceException as e:\n",
    "            print('The was an error...')\n",
    "            print(e)\n",
    "        except Exception as e:\n",
    "            print('The was an error...')\n",
    "            print(e)\n",
    "\n",
    "        return all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_jobs_array(nested_job_data):\n",
    "#     flattened_jobs = []\n",
    "\n",
    "#     for job_group in nested_job_data:\n",
    "#         if job_group is None:\n",
    "#             flattened_jobs.append([None, None, None, None])\n",
    "#             continue\n",
    "\n",
    "#         for job_entry in job_group:\n",
    "#             if job_entry is None:\n",
    "#                 flattened_jobs.append([None, None, None, None])\n",
    "#                 continue\n",
    "\n",
    "#             job_details = []\n",
    "\n",
    "#             for field in job_entry:\n",
    "#                 try:\n",
    "#                     if isinstance(field, WebElement):\n",
    "#                         job_details.append(field)  # Extract and clean text\n",
    "#                     else:\n",
    "#                         job_details.append(field)\n",
    "#                 except Exception:\n",
    "#                     job_details.append(field)\n",
    "\n",
    "#             flattened_jobs.append(job_details)\n",
    "\n",
    "#     return flattened_jobs\n",
    "\n",
    "\n",
    "from src.utils.utils import flatten_jobs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = [\"data scientist\", \"llm engineer\", \"ai engineer\", \"machine learning engineer\", \"mlops engineer\", \"ai developer\", \"generative ai engineer\"]\n",
    "job_location = \"canada\"\n",
    "all_jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for generative ai engineer...\n",
      "Found 5 search bars\n",
      "Found 4 pagination buttons\n",
      "4 pages found\n",
      "button <selenium.webdriver.remote.webelement.WebElement (session=\"bb6a9bbcff1162c4a4dd5ee791e377ff\", element=\"f.FBBCE7F329BA04BFEEDAF0097FBB2F05.d.FE0F3635C8E071E5B9AAC0AD5CAECEDB.e.3198\")>\n",
      "Found 4 pagination buttons\n",
      "The was an error...\n",
      "Message: element not interactable\n",
      "  (Session info: chrome=137.0.7151.122)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6a6e4cda5+78885]\n",
      "\tGetHandleVerifier [0x0x7ff6a6e4ce00+78976]\n",
      "\t(No symbol) [0x0x7ff6a6c099fc]\n",
      "\t(No symbol) [0x0x7ff6a6c61c64]\n",
      "\t(No symbol) [0x0x7ff6a6c53654]\n",
      "\t(No symbol) [0x0x7ff6a6c88b8a]\n",
      "\t(No symbol) [0x0x7ff6a6c52f06]\n",
      "\t(No symbol) [0x0x7ff6a6c88da0]\n",
      "\t(No symbol) [0x0x7ff6a6cb122f]\n",
      "\t(No symbol) [0x0x7ff6a6c88963]\n",
      "\t(No symbol) [0x0x7ff6a6c516b1]\n",
      "\t(No symbol) [0x0x7ff6a6c52443]\n",
      "\tGetHandleVerifier [0x0x7ff6a7124eed+3061101]\n",
      "\tGetHandleVerifier [0x0x7ff6a711f33d+3037629]\n",
      "\tGetHandleVerifier [0x0x7ff6a713e592+3165202]\n",
      "\tGetHandleVerifier [0x0x7ff6a6e6730e+186766]\n",
      "\tGetHandleVerifier [0x0x7ff6a6e6eb3f+217535]\n",
      "\tGetHandleVerifier [0x0x7ff6a6e559b4+114740]\n",
      "\tGetHandleVerifier [0x0x7ff6a6e55b69+115177]\n",
      "\tGetHandleVerifier [0x0x7ff6a6e3c368+10728]\n",
      "\tBaseThreadInitThunk [0x0x7ffc84f1e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffc8543c34c+44]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title in job_titles:\n",
    "    try:\n",
    "        print(f\"Collecting data for {title}...\")\n",
    "        all_jobs = collect_job_data(title, job_location, delay=2)\n",
    "        flattened_jobs = flatten_jobs_array(all_jobs)\n",
    "        columns = [\"Job Title\", \"Company\", \"City\", \"Work Mode\", \"Description\"]\n",
    "\n",
    "        save_to_csv(\n",
    "            data=flattened_jobs, \n",
    "            folder=RAW_FOLDER, \n",
    "            filename=title,\n",
    "            colnames=columns\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while collecting data for {title}:\")\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.close_driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
