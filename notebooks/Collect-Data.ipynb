{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Job info from LinkedIn\n",
    "\n",
    "<a href=\"https://medium.com/nerd-for-tech/linked-in-web-scraper-using-selenium-15189959b3ba\">Tutorial here.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from src.bot.Scraper import Scraper\n",
    "from src.database.csv import save_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FOLDER = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scraper object\n",
    "scraper = Scraper(delay=1)\n",
    "\n",
    "# Login to LinkedIn\n",
    "scraper.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 search bars\n",
      "Found 4 pagination buttons\n"
     ]
    },
    {
     "ename": "ElementNotInteractableException",
     "evalue": "Message: element not interactable\n  (Session info: chrome=137.0.7151.122)\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7fd7fcda5+78885]\n\tGetHandleVerifier [0x0x7ff7fd7fce00+78976]\n\t(No symbol) [0x0x7ff7fd5b99fc]\n\t(No symbol) [0x0x7ff7fd611c64]\n\t(No symbol) [0x0x7ff7fd603654]\n\t(No symbol) [0x0x7ff7fd638b8a]\n\t(No symbol) [0x0x7ff7fd602f06]\n\t(No symbol) [0x0x7ff7fd638da0]\n\t(No symbol) [0x0x7ff7fd66122f]\n\t(No symbol) [0x0x7ff7fd638963]\n\t(No symbol) [0x0x7ff7fd6016b1]\n\t(No symbol) [0x0x7ff7fd602443]\n\tGetHandleVerifier [0x0x7ff7fdad4eed+3061101]\n\tGetHandleVerifier [0x0x7ff7fdacf33d+3037629]\n\tGetHandleVerifier [0x0x7ff7fdaee592+3165202]\n\tGetHandleVerifier [0x0x7ff7fd81730e+186766]\n\tGetHandleVerifier [0x0x7ff7fd81eb3f+217535]\n\tGetHandleVerifier [0x0x7ff7fd8059b4+114740]\n\tGetHandleVerifier [0x0x7ff7fd805b69+115177]\n\tGetHandleVerifier [0x0x7ff7fd7ec368+10728]\n\tBaseThreadInitThunk [0x0x7ffee836259d+29]\n\tRtlUserThreadStart [0x0x7ffee99aaf78+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementNotInteractableException\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m pagination \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjobs-search-pagination__pages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m pagination_buttons \u001b[38;5;241m=\u001b[39m pagination\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//button\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mpagination_buttons\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub\\LinkedIn-Jobs-Analysis\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:94\u001b[0m, in \u001b[0;36mWebElement.click\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclick\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLICK_ELEMENT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub\\LinkedIn-Jobs-Analysis\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub\\LinkedIn-Jobs-Analysis\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:384\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\GitHub\\LinkedIn-Jobs-Analysis\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m: Message: element not interactable\n  (Session info: chrome=137.0.7151.122)\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7fd7fcda5+78885]\n\tGetHandleVerifier [0x0x7ff7fd7fce00+78976]\n\t(No symbol) [0x0x7ff7fd5b99fc]\n\t(No symbol) [0x0x7ff7fd611c64]\n\t(No symbol) [0x0x7ff7fd603654]\n\t(No symbol) [0x0x7ff7fd638b8a]\n\t(No symbol) [0x0x7ff7fd602f06]\n\t(No symbol) [0x0x7ff7fd638da0]\n\t(No symbol) [0x0x7ff7fd66122f]\n\t(No symbol) [0x0x7ff7fd638963]\n\t(No symbol) [0x0x7ff7fd6016b1]\n\t(No symbol) [0x0x7ff7fd602443]\n\tGetHandleVerifier [0x0x7ff7fdad4eed+3061101]\n\tGetHandleVerifier [0x0x7ff7fdacf33d+3037629]\n\tGetHandleVerifier [0x0x7ff7fdaee592+3165202]\n\tGetHandleVerifier [0x0x7ff7fd81730e+186766]\n\tGetHandleVerifier [0x0x7ff7fd81eb3f+217535]\n\tGetHandleVerifier [0x0x7ff7fd8059b4+114740]\n\tGetHandleVerifier [0x0x7ff7fd805b69+115177]\n\tGetHandleVerifier [0x0x7ff7fd7ec368+10728]\n\tBaseThreadInitThunk [0x0x7ffee836259d+29]\n\tRtlUserThreadStart [0x0x7ffee99aaf78+40]\n"
     ]
    }
   ],
   "source": [
    "# Search jobs by title and location\n",
    "job_title = 'data scientist'\n",
    "job_location = 'canada'\n",
    "scraper.search_jobs(job_title, job_location)\n",
    "\n",
    "# Get the  pagination buttons\n",
    "time.sleep(2)\n",
    "pagination_buttons = scraper.get_pagination_buttons()\n",
    "\n",
    "jobs = scraper.get_current_page_jobs()\n",
    "pagination = scraper.driver.find_element(By.CLASS_NAME, \"jobs-search-pagination__pages\")\n",
    "pagination_buttons = pagination.find_elements(By.XPATH, './/button')\n",
    "pagination_buttons[0].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Job Data\n",
    "\n",
    "Search for jobs and loop through all pages to get the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_job_data(job_title, job_location, delay=1):\n",
    "    \n",
    "    # Search jobs by title and location\n",
    "    scraper.search_jobs(job_title, job_location)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Get the  pagination buttons\n",
    "    pagination_buttons = scraper.get_pagination_buttons()\n",
    "    print(f\"{len(pagination_buttons)} pages found\")\n",
    "\n",
    "    # Loop through each page\n",
    "    # [:-1] is to ignore the last page as we click on the i+1 button\n",
    "    all_jobs = []\n",
    "    for i, button in enumerate(pagination_buttons[:-1]):\n",
    "\n",
    "        try:\n",
    "            print(\"button\", button)\n",
    "            # get the jobs of the current page\n",
    "            current_page_jobs = scraper.get_current_page_jobs()\n",
    "            all_jobs.append(current_page_jobs)\n",
    "\n",
    "            # navigate to the next page\n",
    "            # button.click()\n",
    "            # Get the buttons again to avoid a StaleElementReferenceException\n",
    "            pagination_buttons = scraper.get_pagination_buttons()\n",
    "            pagination_buttons[i+1].click()\n",
    "\n",
    "        except StaleElementReferenceException as e:\n",
    "            print('The was an error...')\n",
    "            print(e)\n",
    "        except Exception as e:\n",
    "            print('The was an error...')\n",
    "            print(e)\n",
    "\n",
    "        return all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_jobs_array(nested_job_data):\n",
    "#     flattened_jobs = []\n",
    "\n",
    "#     for job_group in nested_job_data:\n",
    "#         if job_group is None:\n",
    "#             flattened_jobs.append([None, None, None, None])\n",
    "#             continue\n",
    "\n",
    "#         for job_entry in job_group:\n",
    "#             if job_entry is None:\n",
    "#                 flattened_jobs.append([None, None, None, None])\n",
    "#                 continue\n",
    "\n",
    "#             job_details = []\n",
    "\n",
    "#             for field in job_entry:\n",
    "#                 try:\n",
    "#                     if isinstance(field, WebElement):\n",
    "#                         job_details.append(field)  # Extract and clean text\n",
    "#                     else:\n",
    "#                         job_details.append(field)\n",
    "#                 except Exception:\n",
    "#                     job_details.append(field)\n",
    "\n",
    "#             flattened_jobs.append(job_details)\n",
    "\n",
    "#     return flattened_jobs\n",
    "\n",
    "\n",
    "from src.utils.utils import flatten_jobs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = [\"data scientist\", \"llm engineer\", \"ai engineer\", \"machine learning engineer\", \"mlops engineer\", \"ai developer\", \"generative ai engineer\"]\n",
    "job_location = \"canada\"\n",
    "all_jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for data scientist...\n",
      "An error occurred while collecting data for data scientist:\n",
      "name 'scraper' is not defined\n",
      "Collecting data for llm engineer...\n",
      "An error occurred while collecting data for llm engineer:\n",
      "name 'scraper' is not defined\n",
      "Collecting data for ai engineer...\n",
      "An error occurred while collecting data for ai engineer:\n",
      "name 'scraper' is not defined\n",
      "Collecting data for machine learning engineer...\n",
      "An error occurred while collecting data for machine learning engineer:\n",
      "name 'scraper' is not defined\n",
      "Collecting data for mlops engineer...\n",
      "An error occurred while collecting data for mlops engineer:\n",
      "name 'scraper' is not defined\n",
      "Collecting data for ai developer...\n",
      "An error occurred while collecting data for ai developer:\n",
      "name 'scraper' is not defined\n",
      "Collecting data for generative ai engineer...\n",
      "An error occurred while collecting data for generative ai engineer:\n",
      "name 'scraper' is not defined\n"
     ]
    }
   ],
   "source": [
    "for title in job_titles:\n",
    "    try:\n",
    "        print(f\"Collecting data for {title}...\")\n",
    "        all_jobs = collect_job_data(title, job_location, delay=2)\n",
    "        flattened_jobs = flatten_jobs_array(all_jobs)\n",
    "        columns = [\"Job Title\", \"Company\", \"City\", \"Work Mode\", \"Description\"]\n",
    "\n",
    "        save_to_csv(\n",
    "            data=flattened_jobs, \n",
    "            folder=RAW_FOLDER, \n",
    "            filename=title,\n",
    "            colnames=columns\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while collecting data for {title}:\")\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.close_driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
